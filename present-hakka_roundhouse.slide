Hakka Roundhouse
A microservices ready monolith
25 Oct 2018

Loh Siu Yin
Technology Consultant, Beyond Broadcast LLP
siuyin@beyondbroadcast.com

* Hakka Roundhouses

.image hakka-roundhouse.jpg


* Hakka Roundhouse

- Tens of families within a clan live in each house
- Closed to the outside, open on the inside
- Issues within an indivdual house are an internal matter -- "easily" solved
- Issues across roundhouses less easily solved -- war?

---

What are the pros and cons of _software_ monoliths?  

* Microservice ready monolith?

- Can many software modules can "live" within a monolith?
- _Should_ these modules "live" within a single software repository?
- Communications within a monolith is certainly fast and reliable
- But communications with systems outside the monolith relies on the network with ensuing networking lack of guarantees
- As needs demand, can a software module (sub-folder) can be forked perhaps to a separate repo for it to take on a life of its own?

* Can we build micro-service ready monoliths?

- *go* has a built-in internal communication process between goroutines -- go channels

---

- gRPC is a robust, efficient and performant Remote Procedure Call library
- gRPC tools generate *message* types and function stubs for RPCs


- How can we use the message types generate in our goroutines?
- What do we do with gRPC generated *rpc* stubs? Future use?

* Traditional monolith

.play -edit cmd/trad/main.go

---

Why do software monoliths typically grow into an unmanageble mess?
Consider function signatures. Are there any constraints on what they can be?
_Too_ much freedom?

* Hakka Roundhouse monolith

* Hakka Roundhouse monolith

.code cmd/hakka/main.go

Wait a minute! What is this pb stuff? Ignore that for the time being.
What is the function signature? Is there loss of freedom? loss of capability?

* A quick detour -- go modules

* gRPC protocol buffers

- This code uses go modules -- the new dependency management experiment.
The initial go.mod is:

  module github.com/siuyin/present-hakka_roundhouse

- Download, extract and install protoc in you $PATH.

.link https://github.com/google/protobuf/releases

- Tell go modules about the grpc dependencies

  go get google.golang.org/grpc
  go get github.com/golang/protobuf/protoc-gen-go

If you have just cloned a repo, download dependencies with:

  go mod download


* .proto file
- A .proto file defines the contract (interface) between servers and clients.

- In our case we are not (yet) defining a remote procedure call server. We are just going to use protoc generated message definitions.

- Start by thinking about the package name (responsibility / role) you will use to group messages.

I chose package name *arith* (for arithmetic to handle sum, multiply, divide ...).
file: proto/arith/arith.proto

.code proto/arith/arith.proto /10 O/,/20 O/

* Let us generate gRPC message types and function stubs

  protoc -I proto/arith arith.proto --go_out=plugins=grpc:proto/arith

Notes:
 -I : Include these folders when looking for .proto files

arith.proto : The .proto file we are compiling into stub files

--go_out : generate grpc stubs and place them in the proto/arith folder

Let's generate the stub files and look at the results:

  protoc -I proto/arith arith.proto --go_out=plugins=grpc:proto/arith
  ls -lR proto

We can also keep the command string in the source and do:

  go generate cmd/hakka/main.go
  ls -lR proto

* Running our new monolith

.code cmd/hakka/main.go

Demo:

  go run cmd/hakka/main.go

* That's not a service!

* Let's make it a service

Add two random integers and present the results once a second.

.code cmd/hakkasvc/main.go /10 O/,/20 O/

We have two services here:

- A load generator service that generate two random integers
- A sumService which adds the integers

* Who here are new to goroutines and channels?

* Load generator service

.code cmd/hakkasvc/main.go /30 O/,/40 O/

1. Create a request object

2. Send a reference to that object into the channel

* sumService

.code cmd/hakkasvc/main.go /50 O/,/60 O/

1. read request from the channel
2. create a response and send its reference

* Running the service monolith

  go run cmd/hakkasvc/main.go

And it is truly a fully self-contained static binary file

  time CGO_ENABLED=0 go build -o hs -v cmd/hakkasvc/main.go
  file hs
  ls -lhF hs

And it is cross-platform

  time CGO_ENABLED=0 GOOS=windows GOARCH=amd64 go build -o hs.exe -v cmd/hakkasvc/main.go
  file hs.exe
  ls -lhF hs.exe

---

Think about how this single go binary is deployed.
They are like self-contained jars from java but do not need a JVM to run.

* Extracting a microservice

* Making sumService a microservice

Steps:

- Update arith.proto to define the SumService interface
- Implement Sum microservice
- Update monolith to call the new SumService

* arith.proto service definition

.code proto/arith/arith.proto /30 O/,/40 O/

This says:

- The service Arith can have many functions. One of them is called Sum.
- Sum takes a SumRequest message as input.
- Sum returns a SumResponse message as output.

* Sum microservice. File: {servicename}/main.go

.code grpc/arith/main.go /10 O/,/20 O/

1. As usual, we import our generated protocol buffer stub package
2. For convenience, we have our go generate statement in the source as well.
3. We need to define a port as this is now a network call. I have chosen to listen on port 50051 on all interfaces -- that is what ":50051" means.

* Service implementation

gRPC tools only generate program stubs. You still need to write the implementation.

.code grpc/arith/main.go /30 O/,/40 O/

compare with the .proto

.code proto/arith/arith.proto /30 O/,/40 O/

Sum takes a (context and a request) and returns a (response and error status). 

Context?

* Server main()

.code grpc/arith/main.go /50 O/,/60 O/

Let's run the service

  go run grpc/arith/main.go

* Updated monolith sumService

.code cmd/hakkasvc2/main.go /52 O/,/53 O/

gRPC returns both a response and an error status

go forces you to consider how to handle error conditions in the mainline of your code. 
Is this good? bad?

* gRPCSum

.code cmd/hakkasvc2/main.go /70 O/,/80 O/

0, 1: Who do you call?
2: Instantiate _gRPC-generated_ Client and use it to call the service.

* Running the updated monolith

This will now call our new gRPC based Sum service

  go run cmd/hakkasvc2/main.go

---

Now you have two parts that can break.

- The updated hakka roundhouse monolith, which you need to tell where to find the new SumService
- gRPC-based SumService

* Synchronous vs Asynchronous

- RPCs are synchronous like telephone calls.

- Event Sourcing, Event driven systems are asynchronous like email.

---

Which do you prefer?
When do you use on or the other?
These are questions for another time.

But now lets focus on how a monolith can handle events.


* Publish / Subscribe

* Publish / Subcribe with a monolith
Actually pub / sub with a monolith *and* a message broker like NATS.

Pub / Sub enables you to loosely couple components.
A publisher can publish to zero or more subscribers.

Let's reimplement our sum service with pub / sub.

* Pre-requisites - start a message broker

Download NATS, unzip and install it in your $PATH

.link https://github.com/nats-io/gnatsd/releases/download/v1.3.0/gnatsd-v1.3.0-linux-amd64.zip

Now let us run the message broker.

  ~/go/bin/gnatsd

* monolith with NATS

imports

.code cmd/hakkanats/main.go /10 O/,/20 O/

main()

.code cmd/hakkanats/main.go /30 O/,/40 O/

We again have two services, sumService subscriber and a loadGen publisher.

* loadGen

.code cmd/hakkanats/main.go /50 O/,/60 O/

* sumService

.code cmd/hakkanats/main.go /70 O/,/80 O/

Note: subscribe runs a function as a goroutine.

* Running our nats enabled monolith

  go run cmd/hakkanats/main.go


Try stopping then re-starting the NATS message broker.

What do you think will happen?

* presentation and code download

.link https://github.com/siuyin/present-hakka_roundhouse
